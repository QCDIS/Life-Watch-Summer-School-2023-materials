{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e76e26f-3a9b-419b-8c96-1a5a188fc79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import laspy\n",
    "\n",
    "\n",
    "import time\n",
    "import requests\n",
    "                    \n",
    "from dask.distributed import LocalCluster, SSHCluster \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32d454-23ea-4849-a867-8877f7c32dc1",
   "metadata": {},
   "source": [
    "## Global ConfigurationÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73fbe04-b8cc-4d5e-8a94-d54d1750d15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "                    \n",
    "from dask.distributed import LocalCluster, SSHCluster \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote\n",
    "\n",
    "param_username = 'myname'\n",
    "if 'JUPYTERHUB_USER' in os.environ:\n",
    "    param_username = os.environ['JUPYTERHUB_USER']\n",
    "    \n",
    "param_remote_path_root = '/webdav/vl-laserfarm'\n",
    "conf_remote_path_split = pathlib.Path(param_remote_path_root + '/split_'+param_username)\n",
    "conf_remote_path_retiled = pathlib.Path(param_remote_path_root + '/retiled_'+param_username)\n",
    "conf_remote_path_norm = pathlib.Path(param_remote_path_root + '/norm_'+param_username)\n",
    "conf_remote_path_targets = pathlib.Path(param_remote_path_root + '/targets_'+param_username)\n",
    "conf_remote_path_geotiffs = pathlib.Path(param_remote_path_root + '/geotiffs_'+param_username)\n",
    "conf_local_tmp = pathlib.Path('/tmp')\n",
    "conf_remote_path_ahn = pathlib.Path(param_remote_path_root+'/ahn')\n",
    "\n",
    "\n",
    "param_hostname = ''\n",
    "param_login = ''\n",
    "param_password = ''\n",
    "\n",
    "param_feature_name = 'perc_95_normalized_height'\n",
    "param_validate_precision = '0.001'\n",
    "param_tile_mesh_size = '10'\n",
    "param_filter_type= 'select_equal'\n",
    "param_attribute = 'raw_classification'\n",
    "param_min_x = '-113107.81'\n",
    "param_max_x = '398892.19'\n",
    "param_min_y = '214783.87'\n",
    "param_max_y = '726783.87'\n",
    "param_n_tiles_side = '512'\n",
    "param_apply_filter_value = '1'\n",
    "param_laz_compression_factor = '7'\n",
    "param_max_filesize = '262144000'  # desired max file size (in bytes)\n",
    "\n",
    "conf_wd_opts = { 'webdav_hostname': param_hostname, 'webdav_login': param_login, 'webdav_password': param_password}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064b6eb-e738-448e-8561-78b0ad7b3734",
   "metadata": {},
   "source": [
    "## Fetching Laz Files from remote WebDAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf88e5-6d9f-44d1-b5e5-c8e67734f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S1 Fetch Laz Files\n",
    "\n",
    "laz_files = [f for f in list_remote(get_wdclient(conf_wd_opts), conf_remote_path_ahn.as_posix())\n",
    "             if f.lower().endswith('.laz')]\n",
    "print(laz_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cf382-8d3b-4f2a-aa61-f267240709c7",
   "metadata": {},
   "source": [
    "## Splitting big files into smaller ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599581f0-fb85-4d2d-a658-789a4919ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2 split big files \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def save_chunk_to_laz_file(in_filename, \n",
    "                           out_filename, \n",
    "                           offset, \n",
    "                           n_points):\n",
    "    \"\"\"Read points from a LAS/LAZ file and write them to a new file.\"\"\"\n",
    "    \n",
    "    points = np.array([])\n",
    "    \n",
    "    with laspy.open(in_filename) as in_file:\n",
    "        with laspy.open(out_filename, \n",
    "                        mode=\"w\", \n",
    "                        header=in_file.header) as out_file:\n",
    "            in_file.seek(offset)\n",
    "            points = in_file.read_points(n_points)\n",
    "            out_file.write_points(points)\n",
    "    return len(points)\n",
    "\n",
    "def split_strategy(filename, max_filesize):\n",
    "    \"\"\"Set up splitting strategy for a LAS/LAZ file.\"\"\"\n",
    "    with laspy.open(filename) as f:\n",
    "        bytes_per_point = (\n",
    "            f.header.point_format.num_standard_bytes +\n",
    "            f.header.point_format.num_extra_bytes\n",
    "        )\n",
    "        n_points = f.header.point_count\n",
    "    n_points_target = int(\n",
    "        max_filesize * int(param_laz_compression_factor) / bytes_per_point\n",
    "    )\n",
    "    stem, ext = os.path.splitext(filename)\n",
    "    return [\n",
    "        (filename, f\"{stem}-{n}{ext}\", offset, n_points_target)\n",
    "        for n, offset in enumerate(range(0, n_points, n_points_target))\n",
    "    ]\n",
    "\n",
    "from webdav3.client import Client\n",
    "\n",
    "client = Client(conf_wd_opts)\n",
    "client.mkdir(conf_remote_path_split.as_posix())\n",
    "\n",
    "\n",
    "remote_path_split = conf_remote_path_split\n",
    "\n",
    "\n",
    "for file in laz_files:\n",
    "    print('Splitting: '+file)\n",
    "    client.download_sync(remote_path=os.path.join(conf_remote_path_ahn,file), local_path=file)\n",
    "    inps = split_strategy(file, int(param_max_filesize))\n",
    "    for inp in inps:\n",
    "        save_chunk_to_laz_file(*inp)\n",
    "    client.upload_sync(remote_path=os.path.join(conf_remote_path_split,file), local_path=file)\n",
    "\n",
    "    for f in os.listdir('.'):\n",
    "        if not f.endswith('.LAZ'):\n",
    "            continue\n",
    "        os.remove(os.path.join('.', f))\n",
    "    \n",
    "#split_laz_files = laz_files\n",
    "remote_path_retiled = str(conf_remote_path_retiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac19b05-e6be-42ae-8239-0cc7ffbece8f",
   "metadata": {},
   "source": [
    "## Retiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afc563-d4a5-45ba-9546-8aea1c2bd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Read splitted Laz Files\n",
    "remote_path_retiled\n",
    "\n",
    "split_laz_files = [f for f in list_remote(get_wdclient(conf_wd_opts), pathlib.Path(conf_remote_path_split).as_posix())\n",
    "             if f.lower().endswith('.laz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b3708-c431-40a5-9e69-10b3c7777e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S4 Retiling\n",
    "\n",
    "from IPython.display import display, HTML   \n",
    "\n",
    "remote_path_split = conf_remote_path_split\n",
    "\n",
    "grid_retile = {\n",
    "    'min_x': float(param_min_x),\n",
    "    'max_x': float(param_max_x),\n",
    "    'min_y': float(param_min_y),\n",
    "    'max_y': float(param_max_y),\n",
    "    'n_tiles_side': int(param_n_tiles_side)\n",
    "}\n",
    "\n",
    "\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {'tmp_folder': conf_local_tmp.as_posix()},\n",
    "    'pullremote': conf_remote_path_split.as_posix(),\n",
    "    'set_grid': grid_retile,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {},\n",
    "    'pushremote': conf_remote_path_retiled.as_posix(),\n",
    "    'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "#file = split_laz_files[0]    \n",
    "#retiler = Retiler(file,label=file).config(retiling_input).setup_webdav_client(conf_wd_opts)\n",
    "\n",
    "for file in split_laz_files:\n",
    "    print('Retiling: '+file)\n",
    "    retiler = Retiler(file, label=file).config(retiling_input).setup_webdav_client(conf_wd_opts)\n",
    "    retiler_output = retiler.run()\n",
    "    \n",
    "remote_path_norm = str(conf_remote_path_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fdf79-530d-41a7-88c3-1177d1a088b2",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cfd3a-288d-43f2-9f69-298ec0b28751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S5 Fetch Tiles for Norm\n",
    "remote_path_norm\n",
    "\n",
    "tiles = [t.strip('/') for t in list_remote(get_wdclient(conf_wd_opts), conf_remote_path_retiled.as_posix())\n",
    "         if fnmatch.fnmatch(t, 'tile_*_*/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ab2e8-c194-4828-90cd-7d91fe815b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S6 Normalization\n",
    "\n",
    "import copy\n",
    "\n",
    "tiles\n",
    "\n",
    "remote_path_norm = str(conf_remote_path_norm)\n",
    "\n",
    "normalization_input = {\n",
    "    'setup_local_fs': {'tmp_folder': conf_local_tmp.as_posix()},\n",
    "    'pullremote': conf_remote_path_retiled.as_posix(),\n",
    "    'load': {'attributes': 'all'},\n",
    "    # Filter out artifically high points - give overflow error when writing\n",
    "    'apply_filter': {'filter_type':'select_below',\n",
    "                     'attribute': 'z',\n",
    "                     'threshold': 10000.},  # remove non-physically heigh points\n",
    "    'normalize': 1,\n",
    "    'clear_cache' : {},\n",
    "    'pushremote': conf_remote_path_norm.as_posix(),\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('normalize.json', 'w') as f:\n",
    "    json.dump(normalization_input, f)\n",
    "    \n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "#tile = tiles\n",
    "# for tile in tiles:\n",
    "#normalization_input_ = copy.deepcopy(normalization_input)\n",
    "#normalization_input_['export_point_cloud'] = {'filename': '{}.laz'.format(tile),'overwrite': True}\n",
    "#dp = DataProcessing(tile, label=tile).config(normalization_input_).setup_webdav_client(conf_wd_opts)\n",
    "\n",
    "for tile in tiles:\n",
    "    normalization_input_ = copy.deepcopy(normalization_input)\n",
    "    normalization_input_['export_point_cloud'] = {'filename': '{}.laz'.format(tile),'overwrite': True}\n",
    "    dp = DataProcessing(tile, label=tile).config(normalization_input_).setup_webdav_client(conf_wd_opts)\n",
    "    dp.run()\n",
    "\n",
    "remote_path_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa87bfe-cfe8-4aea-a292-e4d11e479dd4",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71329832-64e6-406d-9f03-e1f013754308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S7 Fetch Norm Tiles for feature\n",
    "remote_path_norm\n",
    "\n",
    "tiles = [f for f in list_remote(get_wdclient(conf_wd_opts), pathlib.Path(conf_remote_path_norm).as_posix())\n",
    "             if f.lower().endswith('.laz')]\n",
    "\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbaa1c-9d83-4703-9477-dcd88522ded2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S8 Feature Extraction\n",
    "\n",
    "    \n",
    "features = [\"perc_95_normalized_height\"]\n",
    "\n",
    "tile_mesh_size = float(param_tile_mesh_size)\n",
    "\n",
    "grid_feature = {\n",
    "    'min_x': float(param_min_x),\n",
    "    'max_x': float(param_max_x),\n",
    "    'min_y': float(param_min_y),\n",
    "    'max_y': float(param_max_y),\n",
    "    'n_tiles_side': int(param_n_tiles_side)\n",
    "}\n",
    "\n",
    "feature_extraction_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': (conf_local_tmp / 'tile_input').as_posix(),\n",
    "        'output_folder': (conf_local_tmp / 'tile_output').as_posix(),\n",
    "    },\n",
    "    'pullremote': conf_remote_path_norm.as_posix(),\n",
    "    'load': {'attributes': [param_attribute]},\n",
    "    'normalize': 1,\n",
    "    'apply_filter': {\n",
    "        'filter_type': param_filter_type, \n",
    "        'attribute': param_attribute,\n",
    "        'value': [int(param_apply_filter_value)]#ground surface (2), water (9), buildings (6), artificial objects (26), vegetation (?), and unclassified (1)\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : tile_mesh_size,\n",
    "        'validate' : True,\n",
    "        'validate_precision': float(param_validate_precision),\n",
    "        **grid_feature\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': features,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': tile_mesh_size\n",
    "    },\n",
    "    'export_targets': {\n",
    "        'attributes': features,\n",
    "        'multi_band_files': False\n",
    "    },\n",
    "    'pushremote': conf_remote_path_targets.as_posix(),\n",
    "#     'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "for t in tiles:    \n",
    "    idx = (t.split('.')[0].split('_')[1:])\n",
    "    processing = DataProcessing(t, tile_index=idx,label=t).config(feature_extraction_input).setup_webdav_client(conf_wd_opts)\n",
    "    processing.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eadeb04-efe1-4b68-810b-efd4a91b8c18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GeoTIFF export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5ac0b-78e7-40c8-81ff-0078d3916ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S9 GeoTIFF Export\n",
    "\n",
    "features\n",
    "\n",
    "remote_path_geotiffs = conf_remote_path_geotiffs\n",
    "\n",
    "# setup input dictionary to configure the GeoTIFF export pipeline\n",
    "geotiff_export_input = {\n",
    "    'setup_local_fs': {'tmp_folder': conf_local_tmp.as_posix()},\n",
    "    'pullremote': conf_remote_path_targets.as_posix(),\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': {'xSub': 1, 'ySub': 1},\n",
    "    'create_subregion_geotiffs': {'output_handle': 'geotiff'},\n",
    "    'pushremote': remote_path_geotiffs.as_posix(),\n",
    "    'cleanlocalfs': {}   \n",
    "}\n",
    "\n",
    "writer = GeotiffWriter(input_dir=param_feature_name, bands=param_feature_name,label=param_feature_name).config(geotiff_export_input).setup_webdav_client(conf_wd_opts)\n",
    "writer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea3b0b-8627-44ab-a8da-56d7ce531e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
